{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fbf0f6a9-82db-4a8a-a0d2-989b1405ca73",
   "metadata": {},
   "source": [
    "Having just started learning machine learning and deep learning with the fast.ai course, I wanted to build a small project after finishing lesson 4, as I was starting to feel bored just watching lectures and reading Colab notebooks.\n",
    "\n",
    "After some research I found that one project to get a strong understanding of how it all works was the MNIST classification project, considered the \"Hello, world!\" of computer vision.\n",
    "\n",
    "MNIST is a dataset of handwritten digits developed in 1994 by Yann Lecun, using the works of Corinna Cortes and Chris Burges. It is composed of 60000 digits for the training set, and 10000 digits for the test set.\n",
    "\n",
    "So the goal of this program is to develop a CNN capable of classifying digits. Truthfully, CNN is actually not needed for this particular task as the images are quite small and centered, we could get good results with a normal neural network. But because CNN is to my understanding state of the art in the computer vision field, I wanted to implement it. To do so, I will proceed in the following steps:\n",
    "\n",
    "1 - Load the data, preparation if needed\n",
    "2 - Implement helper functions\n",
    "3 - Implement the CNN architecture -> trying to do it using OOP as it seems like good practice\n",
    "4 - Implement the forward pass -> convolution operation, activation using ReLU, max pooling, repeating, flattening, FC layer using softmax.\n",
    "5 - Implement Backprop using SGD (need more details on that)\n",
    "6 - Implement the training loop, with the number of epochs as a parameter (forward pass -> compute loss -> backward pass -> update params)\n",
    "7 - Get results, test accuracy\n",
    "8 - Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6beced-8fd2-4207-af01-ec45df747aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from scipy.signal import convolve2d\n",
    "import skimage.measure\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfb98f1e-432a-4d43-bdd1-e1873ba929d6",
   "metadata": {},
   "source": [
    "First, we load the MNIST dataset downloaded from Kaggle (https://www.kaggle.com/datasets/hojjatk/mnist-dataset/data).\n",
    "\n",
    "To do so, we use the struct module (https://stackoverflow.com/a/53181925) : the first 8 bytes contain the magic number, used to identify the file type, and the size of the dataset. The next 8 bytes contain the number of rows and the number of columns (28x28).\n",
    "\n",
    "struct.unpack(\">II\", ...) unpacks these bytes into two 32-bit integers (\">II\" means big-endian, two unsigned ints).\n",
    "\n",
    "\n",
    "TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
    "0004     32 bit integer  60000            number of items\n",
    "0008     unsigned byte   ??               label\n",
    "0009     unsigned byte   ??               label\n",
    "........\n",
    "xxxx     unsigned byte   ??               label\n",
    "\n",
    "The labels values are 0 to 9.\n",
    "TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000803(2051) magic number\n",
    "0004     32 bit integer  60000            number of images\n",
    "0008     32 bit integer  28               number of rows\n",
    "0012     32 bit integer  28               number of columns\n",
    "0016     unsigned byte   ??               pixel\n",
    "0017     unsigned byte   ??               pixel\n",
    "........\n",
    "xxxx     unsigned byte   ??               pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d8ba4a-b1ed-489d-926f-43b445b54293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('data/train-images.idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))   \n",
    "    train_data = data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('data/train-labels.idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    train_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))   \n",
    "    print(train_labels)\n",
    "\n",
    "with open('data/t10k-images.idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))   \n",
    "    test_data = data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('data/t10k-labels.idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))   \n",
    "    print(test_labels)\n",
    "\n",
    "plt.imshow(train_data[1],  cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5c657-74ef-4977-83a6-a98c712ae1e6",
   "metadata": {},
   "source": [
    "First, we need to define some helper functions.\n",
    "The ReLU is used after the convolutions to activate them.\n",
    "apply_convolution uses convolve2d of scipy to convolve the image and the kernel.\n",
    "max_pooling uses skimage to pool the image with the pool_size passed as a param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92735cc4-be8d-40de-b394-834c2391d3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def apply_convolution(image, kernel, bias):\n",
    "    return convolve2d(image, kernel, mode='valid') + bias\n",
    "\n",
    "def max_pooling(image, pool_size=(2,2)):\n",
    "    return skimage.measure.block_reduce(image, pool_size, np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653a5cb-c619-4e2c-ad58-fa2dda0829b5",
   "metadata": {},
   "source": [
    "Next, we will build the architecture of our model in OOP.\n",
    "\n",
    "While working on this, a question arised: what is the difference between adding layers of convolution and adding filters per layer of convolution? How can I choose if I do one or the other? Well, adding layers of convolution typically means increasing the depth of the neural network which allows for learning more complex and abstract features of the image. On the other hand, increasing the number of filters of a convolution layer allows for learning more features at the same level of abstraction. The first has a greater impact on computational complexity.\n",
    "Adding more layers : Helpful when dealing with more complex patterns or larger input sizes\n",
    "Adding more filters per layer : Helpful to capture more diverse features at a particular level of abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "159e610a-ea78-4699-a072-1d1a4399ffa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3, 3, 3, 1)\n",
      "(28, 28, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CNN.convolution() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN(ConvLayer(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), ConvLayer(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     55\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m60000\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[203], line 26\u001b[0m, in \u001b[0;36mCNN.forward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(X, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# ReLU\u001b[39;00m\n\u001b[1;32m     28\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(X)\n",
      "\u001b[0;31mTypeError\u001b[0m: CNN.convolution() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "class ConvLayer:\n",
    "\n",
    "    def __init__(self, num_filters, kernel_size, stride=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def initialize(self, input_depth):\n",
    "        self.W = np.random.randn(self.num_filters, self.kernel_size, self.kernel_size, input_depth) / (self.kernel_size * self.kernel_size) # divide xavier initialization\n",
    "        self.b = np.zeros(self.num_filters)\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, *conv_layers):\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        for i, layer in enumerate(self.conv_layers, 1):\n",
    "            print(X.shape[-1])\n",
    "            if layer.W is None:\n",
    "                layer.initialize(X.shape[-1])\n",
    "            print(layer.W.shape)\n",
    "            print(X.shape)\n",
    "            X = self.convolution(X, layer)\n",
    "            X = np.maximum(X, 0)  # ReLU\n",
    "            X = self.max_pool(X)\n",
    "            print(f\"Layer {i} output shape: {X.shape}\")\n",
    "        return X\n",
    "\n",
    "    # For the convolution, I chose to use scipy.signal.convolve2d as it is implemented and compiled in C\n",
    "    # which makes it much faster to operate than if I implemented it myself in python (and it's quite easier too...) \n",
    "    def convolution(self, X, layer):\n",
    "\n",
    "        #example :\n",
    "        #X : (28,28,1)\n",
    "        #layer.W : (3,3,3,1)\n",
    "        #output : (26, 26, 3)\n",
    "        \n",
    "        print(convolve2d\n",
    "        \n",
    "    def max_pool(self, X, pool_size=2):\n",
    "        n, p = X.shape\n",
    "        return X[:n//pool_size*pool_size, :p//pool_size*pool_size].reshape(n//pool_size, pool_size, p//pool_size, pool_size).max(axis=(1,3))\n",
    "\n",
    "cnn = CNN(ConvLayer(3, 3), ConvLayer(1,2))\n",
    "\n",
    "train_data = train_data.reshape(60000,28,28,1)\n",
    "\n",
    "cnn.forward_pass(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19beef-822e-4fde-a4b7-71f5a1c30e93",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## W1 = np.random.randn(5, 5) * 0.01 # 5 by 5 kernels\n",
    "b1 = np.zeros(1)\n",
    "\n",
    "# Convolutional layer 2\n",
    "W2 = np.random.randn(3, 3) * 0.01 # 3 by 3 kernels\n",
    "b2 = np.zeros(1)\n",
    "\n",
    "conv1_output_size = train_data.shape[1] - W1.shape[0] + 1\n",
    "pool1_output_size = conv1_output_size // 2\n",
    "conv2_output_size = pool1_output_size - W2.shape[0] + 1\n",
    "pool2_output_size = conv2_output_size // 2\n",
    "fc_input_size = pool2_output_size * pool2_output_size\n",
    "\n",
    "W3 = np.random.randn(fc_input_size, 10) * 0.01\n",
    "b3 = np.zeros(10)\n",
    "\n",
    "print(W3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d37432-4aea-4002-a341-fe5e59dc01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(image):\n",
    "    \n",
    "    # First convolutional layer\n",
    "    conv1 = apply_convolution(image, W1, b1)\n",
    "    conv1_activated = relu(conv1)\n",
    "    \n",
    "    # First pooling layer\n",
    "    pool1 = max_pooling(conv1_activated)\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    conv2 = apply_convolution(pool1, W2, b2)\n",
    "    conv2_activated = relu(conv2)\n",
    "    \n",
    "    # Second pooling layer\n",
    "    pool2 = max_pooling(conv2_activated)\n",
    "    \n",
    "    # Flatten\n",
    "    flattened = pool2.flatten()\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc = np.dot(flattened, W3) + b3\n",
    "    \n",
    "    # Output layer with softmax\n",
    "    output = softmax(fc.reshape(1, -1))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b3e233d-73d3-46af-a7f5-bbe90cc39048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for first image: [[0.10062846 0.10088258 0.10006423 0.09978168 0.09895976 0.10019187\n",
      "  0.09939027 0.09984605 0.09933434 0.10092076]]\n",
      "Predicted class: 9\n",
      "Actual class: 5\n"
     ]
    }
   ],
   "source": [
    "sample_output = forward_pass(train_data[0])\n",
    "print(\"Output for first image:\", sample_output)\n",
    "print(\"Predicted class:\", np.argmax(sample_output))\n",
    "print(\"Actual class:\", train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aaaa15fa-5823-49c2-90b5-8304fff20c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over all train dataset: 0.105\n"
     ]
    }
   ],
   "source": [
    "all_outputs = np.array([forward_pass(img) for img in train_data[:1000]])\n",
    "\n",
    "results = np.argmax(all_outputs, axis=-1).T\n",
    "\n",
    "print(\"Accuracy over all train dataset:\", np.mean(results == train_labels[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937f800-8dcc-4de7-a652-d000b7f86769",
   "metadata": {},
   "source": [
    "At this point, we achieved the first part of the project. The forward pass allows the model to properly arrive to a conclusion. For now, the result is quite bad due to the randomness of the parameters.\n",
    "\n",
    "Now we need to implement backpropagation to allow the model to learn by computing the loss and updating the weights and biases accordingly.\n",
    "\n",
    "My first idea to get the loss was to compute the difference between the actual number and the confidence for each number. For example:\n",
    "Let's say the actual number is a 2 and the output we got is this [0.09982253 0.10030541 0.1006628  0.09944194 0.09963454 0.10006153\n",
    "  0.10002742 0.10004489 0.09985413 0.10014481] -> the model predicted a 7\n",
    "\n",
    "What we would do is first create an array of size 10 for the actual number with zeros for each index not of that number and a one for the right index : [0 0 1 0 0 0 0 0 0 0]\n",
    "\n",
    "Then we would compute the absolute difference between each value to get the loss for this particular prediction. We would repeat the process for all the 60000 predictions.\n",
    "\n",
    "After some testing, I noticed that the program was really slow to compute the absolute difference for each of the inputs. I then did some research and found out that my solution, while valid, was really slow and was not really commonly used. After some research, I came accross this the Cross Entropy Loss function which is the default loss function to use for multi-class classification problems.\n",
    "\n",
    "\n",
    "\n",
    "So that's what we will use for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b48a47ba-0829-42e8-8b82-c8d81d6ea437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2302550384901763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\narr = np.zeros(10)\\narr[train_labels[0]] = 1\\narr = np.eye(10)[train_labels[0]]\\n\\nloss = abs(np.eye(10)[train_labels] - all_outputs)\\n\\nprint(loss)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true*np.log(y_pred))\n",
    "\n",
    "loss = cross_entropy(np.eye(10)[train_labels], all_outputs)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "\"\"\"\n",
    "arr = np.zeros(10)\n",
    "arr[train_labels[0]] = 1\n",
    "arr = np.eye(10)[train_labels[0]]\n",
    "\n",
    "loss = abs(np.eye(10)[train_labels] - all_outputs)\n",
    "\n",
    "print(loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209c483-e3d7-4926-a233-4e00bf68aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
